{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def r0(Z):\n",
    "    return np.cos(np.sum(Z**2, axis=1))\n",
    "\n",
    "\n",
    "def g0(Z):\n",
    "    return np.prod(np.exp(Z), axis=1)\n",
    "\n",
    "\n",
    "def f0(X):\n",
    "    beta_0 = 1\n",
    "    return X * beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, NNp_ratio, p, q, seed=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # E: R^12, with Ej ~ Uniform[0, 1/2]\n",
    "    E = np.random.uniform(0, 0.5, size=(N+1, q+2))\n",
    "    # Zij = (Ej+1 + rho * E12) / (1 + rho)\n",
    "    rho = 1\n",
    "    Zt = (E[:, 1:11] + rho * E[:, 11].reshape(-1, 1)) / (1 + rho)\n",
    "    Z = Zt[1:, :]\n",
    "    \n",
    "    # target test point\n",
    "    Z0 = Zt[0, :].reshape(1, -1)\n",
    "    # expanded design\n",
    "    Np = int(np.ceil(N / NNp_ratio))\n",
    "    Eps = np.random.normal(0, 1/q, (Np, q))\n",
    "    Zp = Z0 + Eps\n",
    "\n",
    "    # or Xi = r0(Zi) + Vi\n",
    "    V = np.random.normal(0, 1/q, size=(N,))\n",
    "    X = r0(Z) + V\n",
    "\n",
    "    # Yi = f0(Xi) + g0(Z_i) + Ui\n",
    "    U = np.random.normal(0, 1/q, size=(N,))\n",
    "    Y = f0(X) + g0(Z) + U\n",
    "\n",
    "    # Define data for double ML\n",
    "    dml_data = DoubleMLData.from_arrays(Z, Y, X)\n",
    "    \n",
    "\n",
    "    return {\"dml_data\": dml_data, \"Z0\": Z0, \"Zp\": Zp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Round CI Coverage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_r0_round(data, learner_type = \"rf\", alpha=0.05, seed=0):\n",
    "    # Extract data\n",
    "    dml_data = data[\"dml_data\"]\n",
    "    Z = dml_data.x\n",
    "    X = dml_data.d\n",
    "    Z0 = data[\"Z0\"]\n",
    "    Zp = data[\"Zp\"]\n",
    "    p = 1\n",
    "    q = Z.shape[1]\n",
    "    Np = Zp.shape[0]\n",
    "    \n",
    "    # Define learners\n",
    "    if learner_type == \"ols\":\n",
    "        learner = LinearRegression()\n",
    "    elif learner_type == \"rf\":\n",
    "        learner = RandomForestRegressor(n_estimators=q)\n",
    "    elif learner_type == \"nn\":\n",
    "        learner = MLPRegressor( \n",
    "            hidden_layer_sizes=(128, 64, 64, 128),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            max_iter=5000,\n",
    "            random_state=seed,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"This type of learner is not supported yet.\")\n",
    "    \n",
    "    # Define models\n",
    "    g_hat = clone(learner)\n",
    "    r_hat = clone(learner)\n",
    "    \n",
    "    # DoubleML for beta_hat\n",
    "    dml = DoubleMLPLR(dml_data, ml_l=g_hat, ml_m=r_hat, n_folds=10)\n",
    "    dml.fit()\n",
    "    beta_hat = dml.coef\n",
    "    \n",
    "    # Fit r_hat and make predictions\n",
    "    r_hat.fit(Z, X)\n",
    "    X_hat = r_hat.predict(Z)\n",
    "    X_hatp = r_hat.predict(Zp)\n",
    "    \n",
    "    # Correct biases\n",
    "    mean_X_hatp = np.mean(X_hatp)\n",
    "    delta = np.mean(X_hat - X)\n",
    "    sigma_hat1 = np.mean((X_hat - X - delta) ** 2)\n",
    "    sigma_hat2 = np.mean((X_hatp - mean_X_hatp) ** 2)\n",
    "\n",
    "    def w_theta(alpha):\n",
    "        return scipy.stats.norm.ppf(1 - alpha / (2 * p)) * np.sqrt(\n",
    "            sigma_hat1 / N + sigma_hat2 / Np\n",
    "        )\n",
    "\n",
    "    def CI_r0(alpha):\n",
    "        w = w_theta(alpha)\n",
    "        lower = mean_X_hatp - delta - w\n",
    "        upper = mean_X_hatp - delta + w\n",
    "        return [lower, upper]\n",
    "        \n",
    "    def does_CI_cover(CI, truth):\n",
    "        if truth > CI[0] and truth < CI[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return does_CI_cover(CI_r0(alpha), r0(Z0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Interval Coverage Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [1000, 3000, 5000, 7000, 9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_probs_ols = []\n",
    "coverage_probs_rf = []\n",
    "coverage_probs_nn = []\n",
    "for N in Ns:\n",
    "    does_cover_ols = []\n",
    "    does_cover_rf = []\n",
    "    does_cover_nn = []\n",
    "    for seed in range(500):\n",
    "        # Generate data\n",
    "        data = generate_data(N, NNp_ratio=0.1, p=1, q=10, seed=seed)\n",
    "        # Construct CI for r0 and check coverage\n",
    "        ## ols\n",
    "        does_cover_ols.append(CI_r0_round(data, learner_type=\"ols\", seed=seed))\n",
    "        ## rf\n",
    "        does_cover_rf.append(CI_r0_round(data, learner_type=\"rf\", seed=seed))\n",
    "        ## nn\n",
    "        does_cover_nn.append(CI_r0_round(data, learner_type=\"nn\", seed=seed))\n",
    "    coverage_probs_ols.append(np.mean(does_cover_ols))\n",
    "    coverage_probs_rf.append(np.mean(does_cover_rf))\n",
    "    coverage_probs_nn.append(np.mean(does_cover_nn))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))  # Set figure size\n",
    "\n",
    "# Plot multiple lines\n",
    "plt.plot(Ns, coverage_probs_ols, label=\"ols\", linestyle=\"-\")   # Solid line\n",
    "plt.plot(Ns, coverage_probs_rf, label=\"rf\", linestyle=\"--\")  # Dashed line\n",
    "plt.plot(Ns, coverage_probs_nn, label=\"nn\", linestyle=\":\")   # Dotted line\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Coverage Rate\")\n",
    "plt.title(\"Nominal Confidence Level 0.95\")\n",
    "plt.axhline(0.95, color=\"red\", linewidth=0.5)  # Add horizontal line\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".kte_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
