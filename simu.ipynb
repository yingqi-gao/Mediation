{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.stats as stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Numerical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_repetitions = 500\n",
    "\n",
    "rho = 1\n",
    "p = 1\n",
    "q = 10\n",
    "beta_0 = torch.tensor([1])\n",
    "\n",
    "N_range = [100, 300, 500, 700, 900]\n",
    "NNp_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "# Generate noises\n",
    "U = torch.normal(0, 1, size=(N, 1))\n",
    "V = torch.normal(0, 1, size=(N, p))\n",
    "\n",
    "# Generate covariates Zj = (E{j+1}+rho*E5)/(1+rho)\n",
    "Es = torch.rand(N, q)  # uniform [0, 1)\n",
    "Z = (Es + rho * Es[:, 4].view(-1, 1)) / (1 + rho)\n",
    "\n",
    "\n",
    "# Define r0(z)\n",
    "def r0(Z):\n",
    "    return torch.cos(torch.sum(torch.square(Z), dim=1)).view(-1, 1)\n",
    "\n",
    "\n",
    "# Generate X\n",
    "X = r0(Z) + V\n",
    "\n",
    "\n",
    "# Define f0(x)\n",
    "def f0(X):\n",
    "    return X * beta_0\n",
    "\n",
    "\n",
    "# Define g0(z)\n",
    "def g0(Z):\n",
    "    return torch.prod(torch.exp(Z), dim=1).view(-1, 1)\n",
    "\n",
    "\n",
    "# Generate Y\n",
    "Y = f0(X) + g0(Z) + U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(Z, Y, epochs=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model using PyTorch for multi-feature inputs (N, q).\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Independent variable tensor of shape (N, q)\n",
    "    - Y: Dependent variable tensor of shape (N, 1)\n",
    "    - epochs: Number of training iterations\n",
    "    - lr: Learning rate for optimization\n",
    "\n",
    "    Returns:\n",
    "    - trained_model: The trained PyTorch model\n",
    "    \"\"\"\n",
    "    N, q = Z.shape  # Number of samples, number of features\n",
    "\n",
    "    # Define Linear Regression Model\n",
    "    class LinearRegression(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.weights = nn.Linear(input_dim, 1, bias=False)  # No intercept\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.weights(x)\n",
    "\n",
    "    # Initialize model\n",
    "    model = LinearRegression(input_dim=q)\n",
    "\n",
    "    # Loss function (MSE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Optimizer (SGD)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred_Y = model(Z)\n",
    "        loss = criterion(pred_Y, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model  # Return trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn g0 and r0 using linear regressions\n",
    "g0_hat = train_linear_regression(Z, X)\n",
    "r0_hat = train_linear_regression(torch.cat((X, Z), dim=1), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_intervals(model, Z, Y, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Computes prediction intervals for any trained PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model (supports neural networks)\n",
    "    - Z: Independent variable tensor (N, q)\n",
    "    - Y: True dependent variable tensor (N, 1)\n",
    "    - alpha: Significance level (default: 0.05 for 95% confidence)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with predicted values and lower/upper prediction intervals\n",
    "    \"\"\"\n",
    "    N, q = Z.shape  # Number of samples, features\n",
    "\n",
    "    # Get predictions\n",
    "    pred_Y = model(Z).detach()\n",
    "\n",
    "    # Compute residuals\n",
    "    residuals = Y - pred_Y\n",
    "\n",
    "    # Estimate standard deviation of residuals\n",
    "    sigma = torch.sqrt((residuals.T @ residuals) / (N - q - 1))  # df = n-q-1\n",
    "\n",
    "    # Compute Jacobian row-wise (N, q)\n",
    "    def compute_jacobian_rowwise(model, Z):\n",
    "        n, q = Z.shape\n",
    "        jacobian = torch.zeros(n, q)\n",
    "\n",
    "        for i in range(n):\n",
    "            Z_i = Z[i].unsqueeze(0).clone().requires_grad_(True)  # Single row\n",
    "            pred_i = model(Z_i)  # Forward pass for one sample\n",
    "            grad_outputs = torch.ones_like(pred_i)  # Gradient wrt output\n",
    "\n",
    "            # Compute gradient wrt Z_i\n",
    "            grads = torch.autograd.grad(\n",
    "                outputs=pred_i,\n",
    "                inputs=Z_i,\n",
    "                grad_outputs=grad_outputs,\n",
    "                retain_graph=True,\n",
    "                create_graph=False,\n",
    "            )[0]\n",
    "            jacobian[i] = grads.squeeze()  # Store in jacobian matrix\n",
    "\n",
    "        return jacobian\n",
    "\n",
    "    # Compute Jacobian correctly\n",
    "    jacobian = compute_jacobian_rowwise(model, Z)  # Shape (N, q)\n",
    "\n",
    "    # Compute standard errors\n",
    "    se_Y = torch.sqrt(torch.sum(jacobian**2, dim=1, keepdim=True)) * sigma\n",
    "\n",
    "    # Compute critical t-value for prediction intervals\n",
    "    t_critical = stats.t.ppf(1 - alpha / 2, df=N - q - 1)\n",
    "\n",
    "    # Compute 95% prediction intervals\n",
    "    lower_Y = pred_Y - t_critical * se_Y\n",
    "    upper_Y = pred_Y + t_critical * se_Y\n",
    "\n",
    "    # Store results in DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"True_Y\": Y.flatten().numpy(),\n",
    "            \"Pred_Y\": pred_Y.flatten().numpy(),\n",
    "            \"Lower_Y\": lower_Y.flatten().numpy(),\n",
    "            \"Upper_Y\": upper_Y.flatten().numpy(),\n",
    "            \"Covered?\": ((lower_Y <= Y) & (Y <= upper_Y)).flatten().numpy(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.98)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_prediction_intervals(g0_hat, Z, X)[\"Covered?\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kte_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
